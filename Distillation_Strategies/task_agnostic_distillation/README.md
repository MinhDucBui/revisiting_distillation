# Task-Agnostic Distillation 
We build our task-agnostic distillation framework based on  [academic-budget-bert](https//github.com/IntelLabs/academic-budget-bert) which supports pre-training BERT-like models under an academic budget. 

## Data Preprocessing
[academic-budget-bert](https//github.com/IntelLabs/academic-budget-bert) provided a nice [`script`](dataset/) for chunking and preprocessing the Wikipedia and BookCorpus raw text data. 


