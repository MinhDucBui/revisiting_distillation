# Introduction

This repository is taken from [How-to-distill-your-BERT](https://github.com/mainlp/How-to-distill-your-BERT).

# Task-Agnostic Distillation 
We build our task-agnostic distillation framework based on  [academic-budget-bert](https//github.com/IntelLabs/academic-budget-bert) which supports pre-training BERT-like models under an academic budget. 

## Data Preprocessing
[academic-budget-bert](https//github.com/IntelLabs/academic-budget-bert) provided a nice [`script`](dataset/) for chunking and preprocessing the Wikipedia and BookCorpus raw text data. 


